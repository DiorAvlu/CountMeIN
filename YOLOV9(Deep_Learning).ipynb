{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#GPU Test"
      ],
      "metadata": {
        "id": "exeFRb_2P0d5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-WTfJ8mWr5y",
        "outputId": "771a69d2-790b-4414-a0a2-9d47832fdbd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May  9 16:26:36 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Connect Google Drive to Google Colab"
      ],
      "metadata": {
        "id": "b0KdzfzlP7uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwHwHdeAXNYO",
        "outputId": "e78e6a97-cbd9-45b5-b616-c2d4a3af0cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#List files and folders inside the Google Drive"
      ],
      "metadata": {
        "id": "4u8OnTbuQIuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T5QFmffXfkW",
        "outputId": "c9cded3f-7e8f-43d1-cd92-c4d34578459e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Change Directory to the Project Folder"
      ],
      "metadata": {
        "id": "HuZrNF4CQOtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/yolov9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdvLodoMXp6B",
        "outputId": "9555103a-ccdc-4446-bf07-8e0feef7882a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/yolov9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Yolo v9 repository to our Google Drive"
      ],
      "metadata": {
        "id": "SRnxIrVLQTfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/WongKinYiu/yolov9.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1y0mJoeYEL7",
        "outputId": "b72589db-2be2-47d0-db12-e91a8209f999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov9'...\n",
            "remote: Enumerating objects: 668, done.\u001b[K\n",
            "remote: Counting objects: 100% (290/290), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 668 (delta 222), reused 199 (delta 199), pack-reused 378\u001b[K\n",
            "Receiving objects: 100% (668/668), 3.22 MiB | 9.97 MiB/s, done.\n",
            "Resolving deltas: 100% (269/269), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Change Directory to our project folder"
      ],
      "metadata": {
        "id": "fWf8BqAEQXrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov9/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ9UQEV1YGU-",
        "outputId": "5ba4b9d0-4c72-4e59-a28a-aaeb432340ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/yolov9/yolov9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Yolo v9 libraries"
      ],
      "metadata": {
        "id": "y68_5a5-Qepe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "M-MSA7q3YJTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Pre-trained models of Yolo v9"
      ],
      "metadata": {
        "id": "sQ06JpvrQh2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /content/gdrive/MyDrive/yolov9/yolov9 -q https://cdn.pixabay.com/photo/2020/04/18/12/23/woman-5059062_960_720.jpg"
      ],
      "metadata": {
        "id": "VfeGad3PYsTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing the downloaded image on the pre-trained model"
      ],
      "metadata": {
        "id": "v95yuHjuSEKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/gdrive/MyDrive/yolov9/yolov9/runs/train/exp9/weights/best.pt --source /content/gdrive/MyDrive/yolov9/yolov9/Tawjihi.jpeg --device 0"
      ],
      "metadata": {
        "id": "wBNLHxC9Yuhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Display the output result"
      ],
      "metadata": {
        "id": "u2QbyxkTSH8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=f\"/content/gdrive/MyDrive/yolov9/yolov9/runs/detect/exp9/Tawjihi.jpeg\", width=500)"
      ],
      "metadata": {
        "id": "X9XGz7TmcGkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Yolo v9"
      ],
      "metadata": {
        "id": "JgsRyDy5SKvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_dual.py --workers 2 --batch 4  --img 640 --epochs 60 --data /content/gdrive/MyDrive/yolov9/yolov9/data.yaml --weights /content/gdrive/MyDrive/yolov9/yolov9/runs/train/exp8/weights/best.pt --device 0 --cfg /content/gdrive/MyDrive/yolov9/yolov9/models/detect/yolov9_custom.yaml --hyp /content/gdrive/MyDrive/yolov9/yolov9/data/hyps/hyp.scratch-high.yaml"
      ],
      "metadata": {
        "id": "hPuLNlrmglY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71142f0-3f65-4f0c-c89f-1ef96e9ca241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-17 22:13:19.962617: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-17 22:13:19.962669: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-17 22:13:19.964610: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-17 22:13:21.646690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=/content/gdrive/MyDrive/yolov9/yolov9/runs/train/exp8/weights/best.pt, cfg=/content/gdrive/MyDrive/yolov9/yolov9/models/detect/yolov9_custom.yaml, data=/content/gdrive/MyDrive/yolov9/yolov9/data.yaml, hyp=/content/gdrive/MyDrive/yolov9/yolov9/data/hyps/hyp.scratch-high.yaml, epochs=50, batch_size=4, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=2, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=0, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "YOLO 🚀 v0.1-89-g93f1a28 Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 15.6MB/s]\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "\u001b[34m\u001b[1mactivation:\u001b[0m nn.ReLU()\n",
            "  0                -1  1         0  models.common.Silence                   []                            \n",
            "  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
            "  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  3                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
            "  4                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            "  5                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
            "  6                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            "  7                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            "  8                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            "  9                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            " 10                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 7]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
            " 14                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 15           [-1, 5]  1         0  models.common.Concat                    [1]                           \n",
            " 16                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n",
            " 17                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 18          [-1, 13]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n",
            " 20                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 21          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 22                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
            " 23                 5  1    131328  models.common.CBLinear                  [512, [256]]                  \n",
            " 24                 7  1    393984  models.common.CBLinear                  [512, [256, 512]]             \n",
            " 25                 9  1    656640  models.common.CBLinear                  [512, [256, 512, 512]]        \n",
            " 26                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
            " 27                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            " 28                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
            " 29                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 30  [23, 24, 25, -1]  1         0  models.common.CBFuse                    [[0, 0, 0]]                   \n",
            " 31                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
            " 32                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 33      [24, 25, -1]  1         0  models.common.CBFuse                    [[1, 1]]                      \n",
            " 34                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            " 35                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 36          [25, -1]  1         0  models.common.CBFuse                    [[2]]                         \n",
            " 37                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            " 38[31, 34, 37, 16, 19, 22]  1  21542822  models.yolo.DualDDetect                 [1, [512, 512, 512, 256, 512, 512]]\n",
            "yolov9_custom summary: 930 layers, 60797222 parameters, 60797190 gradients, 266.1 GFLOPs\n",
            "\n",
            "Transferred 1412/1412 items from /content/gdrive/MyDrive/yolov9/yolov9/runs/train/exp8/weights/best.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 230 weight(decay=0.0), 247 weight(decay=0.0005), 245 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/yolov9/yolov9/People Detection Dataset/train/labels.cache... 5470 images, 34 backgrounds, 0 corrupt: 100% 5470/5470 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/yolov9/yolov9/People Detection Dataset/valid/labels.cache... 336 images, 1 backgrounds, 0 corrupt: 100% 336/336 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/train/exp9/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp9\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       0/49      4.04G       1.39      1.521      1.777         24        640:   0% 0/1368 [00:11<?, ?it/s]WARNING ⚠️ TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
            "       0/49      6.38G       1.53       1.15       1.58         43        640: 100% 1368/1368 [30:19<00:00,  1.33s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:14<00:00,  2.90it/s]\n",
            "                   all        336        983      0.793      0.774      0.843      0.513\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/49      6.38G      1.457      1.103      1.555          6        640: 100% 1368/1368 [10:49<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.37it/s]\n",
            "                   all        336        983      0.803      0.767      0.831      0.506\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/49      6.38G      1.599       1.22      1.625         21        640: 100% 1368/1368 [10:50<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:13<00:00,  3.21it/s]\n",
            "                   all        336        983      0.817      0.738      0.816       0.48\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/49      6.38G      1.647      1.272      1.645         75        640: 100% 1368/1368 [10:48<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.48it/s]\n",
            "                   all        336        983      0.787      0.721      0.809       0.47\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/49      6.39G      1.651       1.29      1.648         15        640: 100% 1368/1368 [10:53<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.28it/s]\n",
            "                   all        336        983      0.771      0.755      0.813      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/49      6.39G      1.643      1.276      1.652         10        640: 100% 1368/1368 [10:51<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.34it/s]\n",
            "                   all        336        983      0.791      0.691      0.788      0.455\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/49      6.95G      1.631      1.261      1.647         17        640: 100% 1368/1368 [10:54<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.36it/s]\n",
            "                   all        336        983      0.826      0.737      0.827      0.485\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/49      6.95G      1.626      1.263      1.634          4        640: 100% 1368/1368 [11:15<00:00,  2.03it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.48it/s]\n",
            "                   all        336        983      0.834      0.741      0.831      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/49      6.95G      1.603      1.233      1.627         29        640: 100% 1368/1368 [11:03<00:00,  2.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.32it/s]\n",
            "                   all        336        983      0.817      0.732      0.816       0.49\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/49      6.95G       1.61       1.23      1.622         16        640: 100% 1368/1368 [10:53<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.44it/s]\n",
            "                   all        336        983      0.819      0.752      0.841      0.496\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/49      6.95G      1.603      1.229       1.62          9        640: 100% 1368/1368 [10:48<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.43it/s]\n",
            "                   all        336        983      0.804      0.762      0.835      0.505\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/49      6.95G      1.603      1.221      1.627          8        640: 100% 1368/1368 [10:53<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.34it/s]\n",
            "                   all        336        983      0.801      0.737      0.831      0.495\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/49      7.52G      1.566      1.193      1.594         29        640: 100% 1368/1368 [10:44<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.41it/s]\n",
            "                   all        336        983      0.822       0.74      0.838      0.492\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/49      7.52G      1.571      1.192      1.605          3        640: 100% 1368/1368 [10:52<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.36it/s]\n",
            "                   all        336        983      0.833       0.75       0.84      0.498\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/49      7.52G      1.599      1.217       1.63          4        640: 100% 1368/1368 [10:49<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.28it/s]\n",
            "                   all        336        983      0.792      0.742      0.823      0.495\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/49      7.52G      1.596      1.213      1.618         15        640: 100% 1368/1368 [10:50<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 42/42 [00:12<00:00,  3.35it/s]\n",
            "                   all        336        983      0.847       0.74      0.838      0.513\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/49      7.52G       1.61      1.202      1.612         52        640:  51% 693/1368 [05:29<06:46,  1.66it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dislplay Experiment Results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ch4hMdAMVHXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=\"/content/gdrive/MyDrive/yolov9/yolov9/runs/train/exp3/results.png\", width=1000)"
      ],
      "metadata": {
        "id": "mPLNqOFEAXzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test the Train model best.pt on a random image"
      ],
      "metadata": {
        "id": "6pO2fzI9VVdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --img 1280 --conf 0.25 --device 0 --weights /content/gdrive/MyDrive/yolov9/yolov9/yolov9-c.pt --source /content/gdrive/MyDrive/yolov9/yolov9/woman-5059062_960_720.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yO6_E_HBQgP",
        "outputId": "7d6da78d-abc4-4b2f-c99d-26d3a62f9a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/gdrive/MyDrive/yolov9/yolov9/yolov9-c.pt'], source=/content/gdrive/MyDrive/yolov9/yolov9/woman-5059062_960_720.jpg, data=data/coco128.yaml, imgsz=[1280, 1280], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLO 🚀 v0.1-89-g93f1a28 Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 604 layers, 50880768 parameters, 0 gradients, 237.6 GFLOPs\n",
            "image 1/1 /content/gdrive/MyDrive/yolov9/yolov9/woman-5059062_960_720.jpg: 864x1280 1 person, 3 cars, 1 motorcycle, 1 dog, 176.1ms\n",
            "Speed: 1.1ms pre-process, 176.1ms inference, 495.1ms NMS per image at shape (1, 3, 1280, 1280)\n",
            "Results saved to \u001b[1mruns/detect/exp7\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Display the results"
      ],
      "metadata": {
        "id": "MPBOCHgeVaZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(filename=\"/content/gdrive/MyDrive/yolov9/yolov9/runs/detect/exp5/Tawjihi.jpeg\", width=600)"
      ],
      "metadata": {
        "id": "peRJMlL4CQNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import math\n",
        "import os\n",
        "from copy import copy\n",
        "from pathlib import Path\n",
        "from urllib.error import URLError\n",
        "\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from utils import TryExcept, threaded\n",
        "from utils.general import (CONFIG_DIR, FONT, LOGGER, check_font, check_requirements, clip_boxes, increment_path,\n",
        "                           is_ascii, xywh2xyxy, xyxy2xywh)\n",
        "from utils.metrics import fitness\n",
        "from utils.segment.general import scale_image\n",
        "\n",
        "# Settings\n",
        "RANK = int(os.getenv('RANK', -1))\n",
        "matplotlib.rc('font', **{'size': 11})\n",
        "matplotlib.use('Agg')  # for writing to files only\n",
        "\n",
        "\n",
        "class Colors:\n",
        "    # Ultralytics color palette https://ultralytics.com/\n",
        "    def __init__(self):\n",
        "        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n",
        "        hexs = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n",
        "                '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n",
        "        self.palette = [self.hex2rgb(f'#{c}') for c in hexs]\n",
        "        self.n = len(self.palette)\n",
        "\n",
        "    def __call__(self, i, bgr=False):\n",
        "        c = self.palette[int(i) % self.n]\n",
        "        return (c[2], c[1], c[0]) if bgr else c\n",
        "\n",
        "    @staticmethod\n",
        "    def hex2rgb(h):  # rgb order (PIL)\n",
        "        return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
        "\n",
        "\n",
        "colors = Colors()  # create instance for 'from utils.plots import colors'\n",
        "\n",
        "\n",
        "def check_pil_font(font=FONT, size=10):\n",
        "    # Return a PIL TrueType Font, downloading to CONFIG_DIR if necessary\n",
        "    font = Path(font)\n",
        "    font = font if font.exists() else (CONFIG_DIR / font.name)\n",
        "    try:\n",
        "        return ImageFont.truetype(str(font) if font.exists() else font.name, size)\n",
        "    except Exception:  # download if missing\n",
        "        try:\n",
        "            check_font(font)\n",
        "            return ImageFont.truetype(str(font), size)\n",
        "        except TypeError:\n",
        "            check_requirements('Pillow>=8.4.0')  # known issue https://github.com/ultralytics/yolov5/issues/5374\n",
        "        except URLError:  # not online\n",
        "            return ImageFont.load_default()\n",
        "\n",
        "\n",
        "class Annotator:\n",
        "    # YOLOv5 Annotator for train/val mosaics and jpgs and detect/hub inference annotations\n",
        "    def __init__(self, im, line_width=None, font_size=None, font='Arial.ttf', pil=False, example='abc'):\n",
        "        assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to Annotator() input images.'\n",
        "        non_ascii = not is_ascii(example)  # non-latin labels, i.e. asian, arabic, cyrillic\n",
        "        self.pil = pil or non_ascii\n",
        "        if self.pil:  # use PIL\n",
        "            self.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\n",
        "            self.draw = ImageDraw.Draw(self.im)\n",
        "            self.font = check_pil_font(font='Arial.Unicode.ttf' if non_ascii else font,\n",
        "                                       size=font_size or max(round(sum(self.im.size) / 2 * 0.035), 12))\n",
        "        else:  # use cv2\n",
        "            self.im = im\n",
        "        self.lw = line_width or max(round(sum(im.shape) / 2 * 0.003), 2)  # line width\n",
        "\n",
        "    def box_label(self, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):\n",
        "        # Add one xyxy box to image with label\n",
        "        if self.pil or not is_ascii(label):\n",
        "            self.draw.rectangle(box, width=self.lw, outline=color)  # box\n",
        "            if label:\n",
        "                w, h = self.font.getsize(label)  # text width, height\n",
        "                outside = box[1] - h >= 0  # label fits outside box\n",
        "                self.draw.rectangle(\n",
        "                    (box[0], box[1] - h if outside else box[1], box[0] + w + 1,\n",
        "                     box[1] + 1 if outside else box[1] + h + 1),\n",
        "                    fill=color,\n",
        "                )\n",
        "                # self.draw.text((box[0], box[1]), label, fill=txt_color, font=self.font, anchor='ls')  # for PIL>8.0\n",
        "                self.draw.text((box[0], box[1] - h if outside else box[1]), label, fill=txt_color, font=self.font)\n",
        "        else:  # cv2\n",
        "            p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "            cv2.rectangle(self.im, p1, p2, color, thickness=self.lw, lineType=cv2.LINE_AA)\n",
        "            if label:\n",
        "                tf = max(self.lw - 1, 1)  # font thickness\n",
        "                w, h = cv2.getTextSize(label, 0, fontScale=self.lw / 3, thickness=tf)[0]  # text width, height\n",
        "                outside = p1[1] - h >= 3\n",
        "                p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
        "                cv2.rectangle(self.im, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
        "                cv2.putText(self.im,\n",
        "                            label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
        "                            0,\n",
        "                            self.lw / 3,\n",
        "                            txt_color,\n",
        "                            thickness=tf,\n",
        "                            lineType=cv2.LINE_AA)\n",
        "\n",
        "    def masks(self, masks, colors, im_gpu=None, alpha=0.5):\n",
        "        \"\"\"Plot masks at once.\n",
        "        Args:\n",
        "            masks (tensor): predicted masks on cuda, shape: [n, h, w]\n",
        "            colors (List[List[Int]]): colors for predicted masks, [[r, g, b] * n]\n",
        "            im_gpu (tensor): img is in cuda, shape: [3, h, w], range: [0, 1]\n",
        "            alpha (float): mask transparency: 0.0 fully transparent, 1.0 opaque\n",
        "        \"\"\"\n",
        "        if self.pil:\n",
        "            # convert to numpy first\n",
        "            self.im = np.asarray(self.im).copy()\n",
        "        if im_gpu is None:\n",
        "            # Add multiple masks of shape(h,w,n) with colors list([r,g,b], [r,g,b], ...)\n",
        "            if len(masks) == 0:\n",
        "                return\n",
        "            if isinstance(masks, torch.Tensor):\n",
        "                masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "                masks = masks.permute(1, 2, 0).contiguous()\n",
        "                masks = masks.cpu().numpy()\n",
        "            # masks = np.ascontiguousarray(masks.transpose(1, 2, 0))\n",
        "            masks = scale_image(masks.shape[:2], masks, self.im.shape)\n",
        "            masks = np.asarray(masks, dtype=np.float32)\n",
        "            colors = np.asarray(colors, dtype=np.float32)  # shape(n,3)\n",
        "            s = masks.sum(2, keepdims=True).clip(0, 1)  # add all masks together\n",
        "            masks = (masks @ colors).clip(0, 255)  # (h,w,n) @ (n,3) = (h,w,3)\n",
        "            self.im[:] = masks * alpha + self.im * (1 - s * alpha)\n",
        "        else:\n",
        "            if len(masks) == 0:\n",
        "                self.im[:] = im_gpu.permute(1, 2, 0).contiguous().cpu().numpy() * 255\n",
        "            colors = torch.tensor(colors, device=im_gpu.device, dtype=torch.float32) / 255.0\n",
        "            colors = colors[:, None, None]  # shape(n,1,1,3)\n",
        "            masks = masks.unsqueeze(3)  # shape(n,h,w,1)\n",
        "            masks_color = masks * (colors * alpha)  # shape(n,h,w,3)\n",
        "\n",
        "            inv_alph_masks = (1 - masks * alpha).cumprod(0)  # shape(n,h,w,1)\n",
        "            mcs = (masks_color * inv_alph_masks).sum(0) * 2  # mask color summand shape(n,h,w,3)\n",
        "\n",
        "            im_gpu = im_gpu.flip(dims=[0])  # flip channel\n",
        "            im_gpu = im_gpu.permute(1, 2, 0).contiguous()  # shape(h,w,3)\n",
        "            im_gpu = im_gpu * inv_alph_masks[-1] + mcs\n",
        "            im_mask = (im_gpu * 255).byte().cpu().numpy()\n",
        "            self.im[:] = scale_image(im_gpu.shape, im_mask, self.im.shape)\n",
        "        if self.pil:\n",
        "            # convert im back to PIL and update draw\n",
        "            self.fromarray(self.im)\n",
        "\n",
        "    def rectangle(self, xy, fill=None, outline=None, width=1):\n",
        "        # Add rectangle to image (PIL-only)\n",
        "        self.draw.rectangle(xy, fill, outline, width)\n",
        "\n",
        "    def text(self, xy, text, txt_color=(255, 255, 255), anchor='top'):\n",
        "        # Add text to image (PIL-only)\n",
        "        if anchor == 'bottom':  # start y from font bottom\n",
        "            w, h = self.font.getsize(text)  # text width, height\n",
        "            xy[1] += 1 - h\n",
        "        self.draw.text(xy, text, fill=txt_color, font=self.font)\n",
        "\n",
        "    def fromarray(self, im):\n",
        "        # Update self.im from a numpy array\n",
        "        self.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\n",
        "        self.draw = ImageDraw.Draw(self.im)\n",
        "\n",
        "    def result(self):\n",
        "        # Return annotated image as array\n",
        "        return np.asarray(self.im)\n",
        "\n",
        "\n",
        "def feature_visualization(x, module_type, stage, n=32, save_dir=Path('runs/detect/exp')):\n",
        "    \"\"\"\n",
        "    x:              Features to be visualized\n",
        "    module_type:    Module type\n",
        "    stage:          Module stage within model\n",
        "    n:              Maximum number of feature maps to plot\n",
        "    save_dir:       Directory to save results\n",
        "    \"\"\"\n",
        "    if 'Detect' not in module_type:\n",
        "        batch, channels, height, width = x.shape  # batch, channels, height, width\n",
        "        if height > 1 and width > 1:\n",
        "            f = save_dir / f\"stage{stage}_{module_type.split('.')[-1]}_features.png\"  # filename\n",
        "\n",
        "            blocks = torch.chunk(x[0].cpu(), channels, dim=0)  # select batch index 0, block by channels\n",
        "            n = min(n, channels)  # number of plots\n",
        "            fig, ax = plt.subplots(math.ceil(n / 8), 8, tight_layout=True)  # 8 rows x n/8 cols\n",
        "            ax = ax.ravel()\n",
        "            plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "            for i in range(n):\n",
        "                ax[i].imshow(blocks[i].squeeze())  # cmap='gray'\n",
        "                ax[i].axis('off')\n",
        "\n",
        "            LOGGER.info(f'Saving {f}... ({n}/{channels})')\n",
        "            plt.savefig(f, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            np.save(str(f.with_suffix('.npy')), x[0].cpu().numpy())  # npy save\n",
        "\n",
        "\n",
        "def hist2d(x, y, n=100):\n",
        "    # 2d histogram used in labels.png and evolve.png\n",
        "    xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)\n",
        "    hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))\n",
        "    xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)\n",
        "    yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)\n",
        "    return np.log(hist[xidx, yidx])\n",
        "\n",
        "\n",
        "def butter_lowpass_filtfilt(data, cutoff=1500, fs=50000, order=5):\n",
        "    from scipy.signal import butter, filtfilt\n",
        "\n",
        "    # https://stackoverflow.com/questions/28536191/how-to-filter-smooth-with-scipy-numpy\n",
        "    def butter_lowpass(cutoff, fs, order):\n",
        "        nyq = 0.5 * fs\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        return butter(order, normal_cutoff, btype='low', analog=False)\n",
        "\n",
        "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
        "    return filtfilt(b, a, data)  # forward-backward filter\n",
        "\n",
        "\n",
        "def output_to_target(output, max_det=300):\n",
        "    # Convert model output to target format [batch_id, class_id, x, y, w, h, conf] for plotting\n",
        "    targets = []\n",
        "    for i, o in enumerate(output):\n",
        "        box, conf, cls = o[:max_det, :6].cpu().split((4, 1, 1), 1)\n",
        "        j = torch.full((conf.shape[0], 1), i)\n",
        "        targets.append(torch.cat((j, cls, xyxy2xywh(box), conf), 1))\n",
        "    return torch.cat(targets, 0).numpy()\n",
        "\n",
        "\n",
        "@threaded\n",
        "def plot_images(images, targets, paths=None, fname='images.jpg', names=None):\n",
        "    # Plot image grid with labels\n",
        "    if isinstance(images, torch.Tensor):\n",
        "        images = images.cpu().float().numpy()\n",
        "    if isinstance(targets, torch.Tensor):\n",
        "        targets = targets.cpu().numpy()\n",
        "\n",
        "    max_size = 1920  # max image size\n",
        "    max_subplots = 16  # max image subplots, i.e. 4x4\n",
        "    bs, _, h, w = images.shape  # batch size, _, height, width\n",
        "    bs = min(bs, max_subplots)  # limit plot images\n",
        "    ns = np.ceil(bs ** 0.5)  # number of subplots (square)\n",
        "    if np.max(images[0]) <= 1:\n",
        "        images *= 255  # de-normalise (optional)\n",
        "\n",
        "    # Build Image\n",
        "    mosaic = np.full((int(ns * h), int(ns * w), 3), 255, dtype=np.uint8)  # init\n",
        "    for i, im in enumerate(images):\n",
        "        if i == max_subplots:  # if last batch has fewer images than we expect\n",
        "            break\n",
        "        x, y = int(w * (i // ns)), int(h * (i % ns))  # block origin\n",
        "        im = im.transpose(1, 2, 0)\n",
        "        mosaic[y:y + h, x:x + w, :] = im\n",
        "\n",
        "    # Resize (optional)\n",
        "    scale = max_size / ns / max(h, w)\n",
        "    if scale < 1:\n",
        "        h = math.ceil(scale * h)\n",
        "        w = math.ceil(scale * w)\n",
        "        mosaic = cv2.resize(mosaic, tuple(int(x * ns) for x in (w, h)))\n",
        "\n",
        "    # Annotate\n",
        "    fs = int((h + w) * ns * 0.01)  # font size\n",
        "    annotator = Annotator(mosaic, line_width=round(fs / 10), font_size=fs, pil=True, example=names)\n",
        "    for i in range(i + 1):\n",
        "        x, y = int(w * (i // ns)), int(h * (i % ns))  # block origin\n",
        "        annotator.rectangle([x, y, x + w, y + h], None, (255, 255, 255), width=2)  # borders\n",
        "        if paths:\n",
        "            annotator.text((x + 5, y + 5), text=Path(paths[i]).name[:40], txt_color=(220, 220, 220))  # filenames\n",
        "        if len(targets) > 0:\n",
        "            ti = targets[targets[:, 0] == i]  # image targets\n",
        "            boxes = xywh2xyxy(ti[:, 2:6]).T\n",
        "            classes = ti[:, 1].astype('int')\n",
        "            labels = ti.shape[1] == 6  # labels if no conf column\n",
        "            conf = None if labels else ti[:, 6]  # check for confidence presence (label vs pred)\n",
        "\n",
        "            if boxes.shape[1]:\n",
        "                if boxes.max() <= 1.01:  # if normalized with tolerance 0.01\n",
        "                    boxes[[0, 2]] *= w  # scale to pixels\n",
        "                    boxes[[1, 3]] *= h\n",
        "                elif scale < 1:  # absolute coords need scale if image scales\n",
        "                    boxes *= scale\n",
        "            boxes[[0, 2]] += x\n",
        "            boxes[[1, 3]] += y\n",
        "            for j, box in enumerate(boxes.T.tolist()):\n",
        "                cls = classes[j]\n",
        "                color = colors(cls)\n",
        "                cls = names[cls] if names else cls\n",
        "                if labels or conf[j] > 0.25:  # 0.25 conf thresh\n",
        "                    label = f'{cls}' if labels else f'{cls} {conf[j]:.1f}'\n",
        "                    annotator.box_label(box, label, color=color)\n",
        "    annotator.im.save(fname)  # save\n",
        "\n",
        "\n",
        "def plot_lr_scheduler(optimizer, scheduler, epochs=300, save_dir=''):\n",
        "    # Plot LR simulating training for full epochs\n",
        "    optimizer, scheduler = copy(optimizer), copy(scheduler)  # do not modify originals\n",
        "    y = []\n",
        "    for _ in range(epochs):\n",
        "        scheduler.step()\n",
        "        y.append(optimizer.param_groups[0]['lr'])\n",
        "    plt.plot(y, '.-', label='LR')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('LR')\n",
        "    plt.grid()\n",
        "    plt.xlim(0, epochs)\n",
        "    plt.ylim(0)\n",
        "    plt.savefig(Path(save_dir) / 'LR.png', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_val_txt():  # from utils.plots import *; plot_val()\n",
        "    # Plot val.txt histograms\n",
        "    x = np.loadtxt('val.txt', dtype=np.float32)\n",
        "    box = xyxy2xywh(x[:, :4])\n",
        "    cx, cy = box[:, 0], box[:, 1]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 6), tight_layout=True)\n",
        "    ax.hist2d(cx, cy, bins=600, cmax=10, cmin=0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.savefig('hist2d.png', dpi=300)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 6), tight_layout=True)\n",
        "    ax[0].hist(cx, bins=600)\n",
        "    ax[1].hist(cy, bins=600)\n",
        "    plt.savefig('hist1d.png', dpi=200)\n",
        "\n",
        "\n",
        "def plot_targets_txt():  # from utils.plots import *; plot_targets_txt()\n",
        "    # Plot targets.txt histograms\n",
        "    x = np.loadtxt('targets.txt', dtype=np.float32).T\n",
        "    s = ['x targets', 'y targets', 'width targets', 'height targets']\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)\n",
        "    ax = ax.ravel()\n",
        "    for i in range(4):\n",
        "        ax[i].hist(x[i], bins=100, label=f'{x[i].mean():.3g} +/- {x[i].std():.3g}')\n",
        "        ax[i].legend()\n",
        "        ax[i].set_title(s[i])\n",
        "    plt.savefig('targets.jpg', dpi=200)\n",
        "\n",
        "\n",
        "def plot_val_study(file='', dir='', x=None):  # from utils.plots import *; plot_val_study()\n",
        "    # Plot file=study.txt generated by val.py (or plot all study*.txt in dir)\n",
        "    save_dir = Path(file).parent if file else Path(dir)\n",
        "    plot2 = False  # plot additional results\n",
        "    if plot2:\n",
        "        ax = plt.subplots(2, 4, figsize=(10, 6), tight_layout=True)[1].ravel()\n",
        "\n",
        "    fig2, ax2 = plt.subplots(1, 1, figsize=(8, 4), tight_layout=True)\n",
        "    # for f in [save_dir / f'study_coco_{x}.txt' for x in ['yolov5n6', 'yolov5s6', 'yolov5m6', 'yolov5l6', 'yolov5x6']]:\n",
        "    for f in sorted(save_dir.glob('study*.txt')):\n",
        "        y = np.loadtxt(f, dtype=np.float32, usecols=[0, 1, 2, 3, 7, 8, 9], ndmin=2).T\n",
        "        x = np.arange(y.shape[1]) if x is None else np.array(x)\n",
        "        if plot2:\n",
        "            s = ['P', 'R', 'mAP@.5', 'mAP@.5:.95', 't_preprocess (ms/img)', 't_inference (ms/img)', 't_NMS (ms/img)']\n",
        "            for i in range(7):\n",
        "                ax[i].plot(x, y[i], '.-', linewidth=2, markersize=8)\n",
        "                ax[i].set_title(s[i])\n",
        "\n",
        "        j = y[3].argmax() + 1\n",
        "        ax2.plot(y[5, 1:j],\n",
        "                 y[3, 1:j] * 1E2,\n",
        "                 '.-',\n",
        "                 linewidth=2,\n",
        "                 markersize=8,\n",
        "                 label=f.stem.replace('study_coco_', '').replace('yolo', 'YOLO'))\n",
        "\n",
        "    ax2.plot(1E3 / np.array([209, 140, 97, 58, 35, 18]), [34.6, 40.5, 43.0, 47.5, 49.7, 51.5],\n",
        "             'k.-',\n",
        "             linewidth=2,\n",
        "             markersize=8,\n",
        "             alpha=.25,\n",
        "             label='EfficientDet')\n",
        "\n",
        "    ax2.grid(alpha=0.2)\n",
        "    ax2.set_yticks(np.arange(20, 60, 5))\n",
        "    ax2.set_xlim(0, 57)\n",
        "    ax2.set_ylim(25, 55)\n",
        "    ax2.set_xlabel('GPU Speed (ms/img)')\n",
        "    ax2.set_ylabel('COCO AP val')\n",
        "    ax2.legend(loc='lower right')\n",
        "    f = save_dir / 'study.png'\n",
        "    print(f'Saving {f}...')\n",
        "    plt.savefig(f, dpi=300)\n",
        "\n",
        "\n",
        "@TryExcept()  # known issue https://github.com/ultralytics/yolov5/issues/5395\n",
        "def plot_labels(labels, names=(), save_dir=Path('')):\n",
        "    # plot dataset labels\n",
        "    LOGGER.info(f\"Plotting labels to {save_dir / 'labels.jpg'}... \")\n",
        "    c, b = labels[:, 0], labels[:, 1:].transpose()  # classes, boxes\n",
        "    nc = int(c.max() + 1)  # number of classes\n",
        "    x = pd.DataFrame(b.transpose(), columns=['x', 'y', 'width', 'height'])\n",
        "\n",
        "    # seaborn correlogram\n",
        "    sn.pairplot(x, corner=True, diag_kind='auto', kind='hist', diag_kws=dict(bins=50), plot_kws=dict(pmax=0.9))\n",
        "    plt.savefig(save_dir / 'labels_correlogram.jpg', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    # matplotlib labels\n",
        "    matplotlib.use('svg')  # faster\n",
        "    ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)[1].ravel()\n",
        "    y = ax[0].hist(c, bins=np.linspace(0, nc, nc + 1) - 0.5, rwidth=0.8)\n",
        "    with contextlib.suppress(Exception):  # color histogram bars by class\n",
        "        [y[2].patches[i].set_color([x / 255 for x in colors(i)]) for i in range(nc)]  # known issue #3195\n",
        "    ax[0].set_ylabel('instances')\n",
        "    if 0 < len(names) < 30:\n",
        "        ax[0].set_xticks(range(len(names)))\n",
        "        ax[0].set_xticklabels(list(names.values()), rotation=90, fontsize=10)\n",
        "    else:\n",
        "        ax[0].set_xlabel('classes')\n",
        "    sn.histplot(x, x='x', y='y', ax=ax[2], bins=50, pmax=0.9)\n",
        "    sn.histplot(x, x='width', y='height', ax=ax[3], bins=50, pmax=0.9)\n",
        "\n",
        "    # rectangles\n",
        "    labels[:, 1:3] = 0.5  # center\n",
        "    labels[:, 1:] = xywh2xyxy(labels[:, 1:]) * 2000\n",
        "    img = Image.fromarray(np.ones((2000, 2000, 3), dtype=np.uint8) * 255)\n",
        "    for cls, *box in labels[:1000]:\n",
        "        ImageDraw.Draw(img).rectangle(box, width=1, outline=colors(cls))  # plot\n",
        "    ax[1].imshow(img)\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    for a in [0, 1, 2, 3]:\n",
        "        for s in ['top', 'right', 'left', 'bottom']:\n",
        "            ax[a].spines[s].set_visible(False)\n",
        "\n",
        "    plt.savefig(save_dir / 'labels.jpg', dpi=200)\n",
        "    matplotlib.use('Agg')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def imshow_cls(im, labels=None, pred=None, names=None, nmax=25, verbose=False, f=Path('images.jpg')):\n",
        "    # Show classification image grid with labels (optional) and predictions (optional)\n",
        "    from utils.augmentations import denormalize\n",
        "\n",
        "    names = names or [f'class{i}' for i in range(1000)]\n",
        "    blocks = torch.chunk(denormalize(im.clone()).cpu().float(), len(im),\n",
        "                         dim=0)  # select batch index 0, block by channels\n",
        "    n = min(len(blocks), nmax)  # number of plots\n",
        "    m = min(8, round(n ** 0.5))  # 8 x 8 default\n",
        "    fig, ax = plt.subplots(math.ceil(n / m), m)  # 8 rows x n/8 cols\n",
        "    ax = ax.ravel() if m > 1 else [ax]\n",
        "    # plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "    for i in range(n):\n",
        "        ax[i].imshow(blocks[i].squeeze().permute((1, 2, 0)).numpy().clip(0.0, 1.0))\n",
        "        ax[i].axis('off')\n",
        "        if labels is not None:\n",
        "            s = names[labels[i]] + (f'—{names[pred[i]]}' if pred is not None else '')\n",
        "            ax[i].set_title(s, fontsize=8, verticalalignment='top')\n",
        "    plt.savefig(f, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    if verbose:\n",
        "        LOGGER.info(f\"Saving {f}\")\n",
        "        if labels is not None:\n",
        "            LOGGER.info('True:     ' + ' '.join(f'{names[i]:3s}' for i in labels[:nmax]))\n",
        "        if pred is not None:\n",
        "            LOGGER.info('Predicted:' + ' '.join(f'{names[i]:3s}' for i in pred[:nmax]))\n",
        "    return f\n",
        "\n",
        "\n",
        "def plot_evolve(evolve_csv='/content/Final_Result.csv'):  # from utils.plots import *; plot_evolve()\n",
        "    # Plot evolve.csv hyp evolution results\n",
        "    evolve_csv = Path(evolve_csv)\n",
        "    data = pd.read_csv(evolve_csv)\n",
        "    keys = [x.strip() for x in data.columns]\n",
        "    x = data.values\n",
        "    f = fitness(x)\n",
        "    j = np.argmax(f)  # max fitness index\n",
        "    plt.figure(figsize=(10, 12), tight_layout=True)\n",
        "    matplotlib.rc('font', **{'size': 8})\n",
        "    print(f'Best results from row {j} of {evolve_csv}:')\n",
        "    for i, k in enumerate(keys[7:]):\n",
        "        v = x[:, 7 + i]\n",
        "        mu = v[j]  # best single result\n",
        "        plt.subplot(6, 5, i + 1)\n",
        "        plt.scatter(v, f, c=hist2d(v, f, 20), cmap='viridis', alpha=.8, edgecolors='none')\n",
        "        plt.plot(mu, f.max(), 'k+', markersize=15)\n",
        "        plt.title(f'{k} = {mu:.3g}', fontdict={'size': 9})  # limit to 40 characters\n",
        "        if i % 5 != 0:\n",
        "            plt.yticks([])\n",
        "        print(f'{k:>15}: {mu:.3g}')\n",
        "    f = evolve_csv.with_suffix('.png')  # filename\n",
        "    plt.savefig(f, dpi=200)\n",
        "    plt.close()\n",
        "    print(f'Saved {f}')\n",
        "\n",
        "\n",
        "def plot_results(file='/content/Final_Result.csv', dir=''):\n",
        "    # Plot training results.csv. Usage: from utils.plots import *; plot_results('path/to/results.csv')\n",
        "    save_dir = Path(file).parent if file else Path(dir)\n",
        "    fig, ax = plt.subplots(2, 5, figsize=(12, 6), tight_layout=True)\n",
        "    ax = ax.ravel()\n",
        "    files = list(save_dir.glob('results*.csv'))\n",
        "    assert len(files), f'No results.csv files found in {save_dir.resolve()}, nothing to plot.'\n",
        "    for f in files:\n",
        "        try:\n",
        "            data = pd.read_csv(f)\n",
        "            s = [x.strip() for x in data.columns]\n",
        "            x = data.values[:, 0]\n",
        "            for i, j in enumerate([1, 2, 3, 4, 5, 8, 9, 10, 6, 7]):\n",
        "                y = data.values[:, j].astype('float')\n",
        "                # y[y == 0] = np.nan  # don't show zero values\n",
        "                ax[i].plot(x, y, marker='.', label=f.stem, linewidth=2, markersize=8)\n",
        "                ax[i].set_title(s[j], fontsize=12)\n",
        "                # if j in [8, 9, 10]:  # share train and val loss y axes\n",
        "                #     ax[i].get_shared_y_axes().join(ax[i], ax[i - 5])\n",
        "        except Exception as e:\n",
        "            LOGGER.info(f'Warning: Plotting error for {f}: {e}')\n",
        "    ax[1].legend()\n",
        "    fig.savefig(save_dir / 'results.png', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def profile_idetection(start=0, stop=0, labels=(), save_dir=''):\n",
        "    # Plot iDetection '*.txt' per-image logs. from utils.plots import *; profile_idetection()\n",
        "    ax = plt.subplots(2, 4, figsize=(12, 6), tight_layout=True)[1].ravel()\n",
        "    s = ['Images', 'Free Storage (GB)', 'RAM Usage (GB)', 'Battery', 'dt_raw (ms)', 'dt_smooth (ms)', 'real-world FPS']\n",
        "    files = list(Path(save_dir).glob('frames*.txt'))\n",
        "    for fi, f in enumerate(files):\n",
        "        try:\n",
        "            results = np.loadtxt(f, ndmin=2).T[:, 90:-30]  # clip first and last rows\n",
        "            n = results.shape[1]  # number of rows\n",
        "            x = np.arange(start, min(stop, n) if stop else n)\n",
        "            results = results[:, x]\n",
        "            t = (results[0] - results[0].min())  # set t0=0s\n",
        "            results[0] = x\n",
        "            for i, a in enumerate(ax):\n",
        "                if i < len(results):\n",
        "                    label = labels[fi] if len(labels) else f.stem.replace('frames_', '')\n",
        "                    a.plot(t, results[i], marker='.', label=label, linewidth=1, markersize=5)\n",
        "                    a.set_title(s[i])\n",
        "                    a.set_xlabel('time (s)')\n",
        "                    # if fi == len(files) - 1:\n",
        "                    #     a.set_ylim(bottom=0)\n",
        "                    for side in ['top', 'right']:\n",
        "                        a.spines[side].set_visible(False)\n",
        "                else:\n",
        "                    a.remove()\n",
        "        except Exception as e:\n",
        "            print(f'Warning: Plotting error for {f}; {e}')\n",
        "    ax[1].legend()\n",
        "    plt.savefig(Path(save_dir) / 'idetection_profile.png', dpi=200)\n",
        "\n",
        "\n",
        "def save_one_box(xyxy, im, file=Path('im.jpg'), gain=1.02, pad=10, square=False, BGR=False, save=True):\n",
        "    # Save image crop as {file} with crop size multiple {gain} and {pad} pixels. Save and/or return crop\n",
        "    xyxy = torch.tensor(xyxy).view(-1, 4)\n",
        "    b = xyxy2xywh(xyxy)  # boxes\n",
        "    if square:\n",
        "        b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # attempt rectangle to square\n",
        "    b[:, 2:] = b[:, 2:] * gain + pad  # box wh * gain + pad\n",
        "    xyxy = xywh2xyxy(b).long()\n",
        "    clip_boxes(xyxy, im.shape)\n",
        "    crop = im[int(xyxy[0, 1]):int(xyxy[0, 3]), int(xyxy[0, 0]):int(xyxy[0, 2]), ::(1 if BGR else -1)]\n",
        "    if save:\n",
        "        file.parent.mkdir(parents=True, exist_ok=True)  # make directory\n",
        "        f = str(increment_path(file).with_suffix('.jpg'))\n",
        "        # cv2.imwrite(f, crop)  # save BGR, https://github.com/ultralytics/yolov5/issues/7007 chroma subsampling issue\n",
        "        Image.fromarray(crop[..., ::-1]).save(f, quality=95, subsampling=0)  # save RGB\n",
        "    return crop\n",
        "\n",
        "plot_evolve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "DydsJKlVlMWs",
        "outputId": "c67c7e1d-cd32-4d3a-8adf-3328ac1b2468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.plot_evolve(evolve_csv='/content/Final_Result.csv')>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>plot_evolve</b><br/>def plot_evolve(evolve_csv=&#x27;/content/Final_Result.csv&#x27;)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/gdrive/MyDrive/yolov9/yolov9/&lt;ipython-input-30-76b8eacef113&gt;</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}